{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLHW_Week2_Qns2_and_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te-emYgOoqz-",
        "colab_type": "text"
      },
      "source": [
        "# Qns 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA2ivuxhBUP9",
        "colab_type": "text"
      },
      "source": [
        "## Find out which distance measure, Euclidean, Manhattan or Maximum absolute, was used for each figure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePrOT-vcCTIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# our origin\n",
        "origin = np.zeros((1,2))\n",
        "# setting up each of the coordinate in a 3 x 2 matrice\n",
        "vector_bunch = np.array([-5, 2, 4, 4, 0, -6]).reshape(3,2)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKDyC_AZonoR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns which vector was closest to origin based on each distance type\n",
        "def man_euc_max(origin, bunch_of_vectors):\n",
        "\n",
        "  man_distances = np.linalg.norm(bunch_of_vectors - origin, 1, axis=1)  \n",
        "  euc_distances = np.linalg.norm(bunch_of_vectors - origin, axis=1)\n",
        "  max_distances = np.linalg.norm(bunch_of_vectors - origin, np.inf, axis=1)\n",
        "  \n",
        "  # find which coordinate is closest based on euclidean\n",
        "  index_min = np.argmin(man_distances)\n",
        "  closest_coord = bunch_of_vectors[index_min]\n",
        "  print(\"The coordinate closest to origin based on Manhattan Distance is:\")\n",
        "  print(closest_coord)\n",
        "  \n",
        "  index_min = np.argmin(euc_distances)\n",
        "  closest_coord = bunch_of_vectors[index_min]\n",
        "  print(\"The coordinate closest to origin based on Euclidean Distance is:\")\n",
        "  print(closest_coord)\n",
        "  \n",
        "  index_min = np.argmin(max_distances)\n",
        "  closest_coord = bunch_of_vectors[index_min]\n",
        "  print(\"The coordinate closest to origin based on Max Absolute Distance is:\")\n",
        "  print(closest_coord)\n",
        "  \n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmr2SfLcE35w",
        "colab_type": "code",
        "outputId": "44803e8f-77a7-4d1b-fc52-a7d5413881f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "man_euc_max(origin, vector_bunch)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The coordinate closest to origin based on Manhattan Distance is:\n",
            "[ 0 -6]\n",
            "The coordinate closest to origin based on Euclidean Distance is:\n",
            "[-5  2]\n",
            "The coordinate closest to origin based on Max Absolute Distance is:\n",
            "[4 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuY11K37G1Hb",
        "colab_type": "text"
      },
      "source": [
        "## Given the above results, we can derive for each of the graphs:\n",
        "\n",
        "### Graph A: Max Absolute Distance Measure\n",
        "\n",
        "### Graph B: Euclidean Distance Measure\n",
        "\n",
        "### Graph C: Manhattan Distance Measure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URaVGS4jHbJR",
        "colab_type": "text"
      },
      "source": [
        "# Qns 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxcrS8fhHdrD",
        "colab_type": "text"
      },
      "source": [
        "## Advantages and Disadvantages of K Medoids compared to K Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6kxyuUKLeeb",
        "colab_type": "text"
      },
      "source": [
        "An advantage of K Medoids is it's greater robustness to noise and outliers compared to K Means. This is owed to the fact that K Medoids is based off medoids, points that lie within the dataset of choice, where the distance being minimized is the absolute distance between points and the centroid rather than the squared distance\n",
        "\n",
        "However on the downside, as K Medoids is most commonly implemented using PAM (Partitioning Around Medoids) algorithm, it uses a greedy algorithm which may not always find the global optimal solution. Compared to K Means, it is computationally slower as K Medoids has a time complexity of O(k * ((n-k) ** 2)) while K Means has a time complexity of O(nk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1MkSiogWbgL",
        "colab_type": "text"
      },
      "source": [
        "## Advantages and Disadvantages of K Means compared to K Medoids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvuoXobOWgwx",
        "colab_type": "text"
      },
      "source": [
        "One main advantage K Means has over K Medoids is its computation speed. As mentioned above, it has a better time complexity as it only spend O(n) for assigning each datapoint to a centre, and O(nk) for calculating the mean for each centre. Hence O(nk), which is faster than K Medoids which requires a greater amount of computation power simply because of repositioning of the exemplar in every iteration through swapping.\n",
        "\n",
        "In contrast to K Medoids, K Means should only be used with distances that are consistent with the mean as there may be potential failure in convergence, whereas for K Medoids, it can be used with any similarity measure.\n",
        "\n",
        "Another thing about K Means that was shown in this Homework was that clusters can disappear while using K Means. In a situation where a fixed number of clusters is required, K Means would lose out to K Medoids as the latter will not have a situation where clusters disappear due to the fact that its clusters are centred around the medoids which are fixed datapoints within the given dataset. \n",
        "\n",
        "In a situation where finding the optimal number of clusters is required, however, K Means would win K Medoids\n",
        "\n"
      ]
    }
  ]
}